#!/bin/bash
echo "in order to run this script you need to install some tools"
url=$1


echo "cloud enumeration"
touch clouds.txt
cloud_enum -k $1 >clouds.txt

echo "subdomain enumeration and brute force"
touch subdomains1.txt subdomains.txt
/opt/dnscan/dnscan.py -d %%.$1 -n -N | tee -a subdomains1.txt
assetfinder $1 | tee -a subdomains1.txt
amass enum -d $1 -passive | tee -a subdomains1.txt 

cat subdomains1.txt |sort -u | grep ".$1"| sed '/ /d' > subdomains.txt
rm subdomains1.txt


echo "alive subdomains"
touch alivesubs.txt
cat subdomains.txt | httprobe > alivesubs.txt


echo "pull out the ips and delete https/http"
cat alivesubs.txt | sed 's|http://||g' | sed 's|https://||g' | sort -u > newalivesubs.txt
dig -f newalivesubs.txt +short | sort -u | sed '/[a-z]/d' > ips.txt
rm newalivesubs.txt

echo "scan for ports"
touch scannedsubs.txt
nmap -iL ips.txt -T4 -F -oA scannedsubs.txt
rm ips.txt

echo "past information"
touch waybackinfo.txt
cat subdomains | waybackurls| sort -u > waybackinfo.txt

echo "parameter pulling"
touch wayback_paramas.txt
cat waybackinfo.txt| grep "?*="| cut -d "=" -f 1 | sort -u > wayback_params.txt

echo "js/jsp/json/php/aspx/html information"
touch scrape_data.txt
awk '/.js/ {print}' waybackinfo.txt > scrape_data.txt
awk '/.php/ {print}' waybackinfo.txt >> scrape_data.txt
awk '/.aspx/ {print}' waybackinfo.txt >> scrape_data.txt
awk '/.html/ {print}' waybackinfo.txt >> scrape_data.txt

echo "subdomain enumeration on alives subdomains"

for URL in $(<alivesubs.txt); do
   ffuf -w /usr/share/wordlists/seclists/SecLists-master/Discovery/Web-Content/directory-list-2.3-small.txt -u "$URL"/FUZZ -mc 200 >>dirbust.txt
done
